{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introductory code: configure this notebook to display all output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introductory code: import functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk, re, pprint\n",
    "from nltk import word_tokenize\n",
    "from nltk import sent_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Open a transcript file called \"text1.txt\": this should be saved in the current working directory. The next few lines then \"tokenise\" this, i.e. convert into the right format for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref = open('text1.txt')\n",
    "type(ref)\n",
    "raw = ref.read()\n",
    "type(raw)\n",
    "tokens = word_tokenize(raw)\n",
    "type(tokens)\n",
    "\n",
    "text = nltk.Text(tokens)\n",
    "type(text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Produce a dispersion plot of some key words (can change the words as you like, e.g. include key clinical terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text.dispersion_plot([\"name\", \"pain\", \"medicine\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look at collocations (words that frequently occur beside each other)\n",
    "#### look at words that appear in a similar context to 'pain'\n",
    "#### look at shared contexts between the words medicine and pain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Collocations\")\n",
    "text.collocations()\n",
    "\n",
    "print()\n",
    "print(\"Words in similar contexts\")\n",
    "text.similar(\"pain\")\n",
    "\n",
    "print()\n",
    "print(\"Common contexts\")\n",
    "text.common_contexts([\"pressure\", \"pain\"])\n",
    "\t#How to examine just the contexts that are SHARED by TWO or more words?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examine amount of vocabulary and the repetition of vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Length of text\")\n",
    "len(text)\n",
    "\t#includes punctuation\n",
    "\t\n",
    "\n",
    "print()\n",
    "print(\"Size of vocabulary\")\n",
    "vocab_size = len(set(text))\n",
    "\t#size of vocabulary.\n",
    "vocab_size\n",
    "\n",
    "print()\n",
    "print(\"Lexical richness\")\n",
    "len(set(text)) / len(text)\n",
    "\t#LEXical richness.\n",
    "\t\n",
    "print(\"What percentage of the text is taken up by a specific word?\")\n",
    "freq_word = 100 * text.count('a') / len(text)\n",
    "freq_word\n",
    "\t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look at the most common words, the frequency of specific words; and words that only appear once ('hapaxes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist1 = nltk.FreqDist(text)  \n",
    "print(fdist1)  \n",
    "    ###<FreqDist with 19317 samples and 260819 outcomes>\n",
    "fdist1.most_common(50) \n",
    "\t#print 50 x most common tokens\n",
    "\t\t\n",
    "fdist1['doctor']\n",
    "\t#freq of particular word.\n",
    "\n",
    "fdist1.plot(50, cumulative=True)\n",
    "\t#cumulative frequency plot for top 50 words\n",
    "\t\n",
    "\t\n",
    "fdist1.hapaxes()\n",
    "\t#hapaxes ie words occuring only once\n",
    "\t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Produce a list of modal words (e.g. 'could', 'may', 'might') and count their frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist = nltk.FreqDist(w.lower() for w in text)\n",
    "modals = ['can', 'could', 'may', 'might', 'must', 'will']\n",
    "for m in modals:\n",
    "    print(m + ':', fdist[m], end=' ')\n",
    "\n",
    "\t\t\n",
    "\t\t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count the number of questions: assume that each '?' indicates one question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in ['?']:\n",
    "    print('Number of questions: ', fdist[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming versus Lemmatisation (part 1: stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\t##### STEMMING V LEMMATISATION:\n",
    "porter = nltk.PorterStemmer()\n",
    "lancaster = nltk.LancasterStemmer()\n",
    "porter_stems = [porter.stem(t) for t in tokens]\n",
    "\n",
    "##### First 10 items in list of porter stems.\n",
    "porter_stems[:10]\n",
    "\n",
    "#####  50 most common items in porter stem frequency distribution.\n",
    "porter_freq = nltk.FreqDist(porter_stems)\n",
    "porter_freq.most_common(50) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lancaster_stems = [lancaster.stem(t) for t in tokens]\n",
    "\n",
    "##### First 10 items in list of porter stems.\n",
    "lancaster_stems[:10]\n",
    "\n",
    "#####  50 most common items in lancaster stem frequency distribution.\n",
    "lancaster_freq = nltk.FreqDist(lancaster_stems)\n",
    "lancaster_freq.most_common(50) \n",
    "\n",
    "\"\"\"\n",
    "    Stemming = strip off the affixes (e.g. \"lying\" becomes \"lie\" // government becomes govern // strange becomes strang)\n",
    "\tHere we try 2 of them. Porter & Lancaster.\n",
    "\tStemming is NOT a well-defined process, and we typically pick the stemmer that best suits the application we have in mind. \n",
    "    The Porter Stemmer is a good choice if you are indexing some texts and want to SUPPORT SEARCH USING ALTERNATIVE FORMS OF WORDS.\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming versus Lemmatisation (part 2: lemmatisation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"50 x first lemmas in the text.\")\n",
    "wnl = nltk.WordNetLemmatizer()\n",
    "wnl_list = [wnl.lemmatize(t) for t in tokens]\n",
    "wnl_list[:50]\n",
    "\n",
    "print()\n",
    "print(\"50 x most common lemmas\")\n",
    "wnl_freq = nltk.FreqDist(wnl_list)\n",
    "wnl_freq.most_common(50)\n",
    "\n",
    "\"\"\"\n",
    "\tLemmatisation = ensure the word is recognised in the dictionary (lemma / headword)\n",
    "\tThe WordNet lemmatizer is a good choice if you want to compile the vocabulary of some texts and want a list of valid lemmas (or lexicon headwords).\n",
    "\"\"\"\n",
    "\t\n",
    "\t\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attach the Part of speech to each word (e.g. nouns, adjectives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### list of tuples: each tuple contains a word & pos\n",
    "text_pos = nltk.pos_tag(text)\n",
    "type(text_pos)\n",
    "text_pos[:10]\n",
    "\n",
    "#### convert list of tuples into Dictionary: keys are unique, values needn't be.\n",
    "text_pos_dict = dict(text_pos)\n",
    "type(text_pos_dict)\n",
    "text_pos_dict\n",
    "\n",
    "\t#get info on any POS tags.\n",
    "nltk.help.upenn_tagset()\n",
    "nltk.help.upenn_tagset(\"JJ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count the frequency of each part of speech: e.g. adjectives and adverbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_count = Counter(text_pos_dict.values())\n",
    "pos_count\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create separate dictionaries for some parts of speech: facilitate visual scrutiny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\")\n",
    "print(\"CC coordinating conjunction (and but either both neither)\")\n",
    "coordinating_conjunction_dict = {k : v for k,v in text_pos_dict.items() if v in [\"CC\"]}\n",
    "coordinating_conjunction_dict\n",
    "coordinating_conjunction_list = list(coordinating_conjunction_dict.keys())\n",
    "coordinating_conjunction_list\n",
    "\n",
    "\n",
    "print(\"\")\n",
    "print(\"JJ JJR JJS  adjective\")\n",
    "adjective_dict                = {k : v for k,v in text_pos_dict.items() if v in [\"JJ\"] or v in [\"JJR\"] or v in [\"JJS\"]}\n",
    "adjective_dict\n",
    "adjective_list = list(adjective_dict.keys())\n",
    "adjective_list\n",
    "\n",
    "\n",
    "print(\"\")\n",
    "print(\"NN NNP NNPS NNS noun\")\n",
    "noun_dict                = {k : v for k,v in text_pos_dict.items() if v in [\"NN\"] or v in [\"NNP\"] or v in [\"NNPS\"] or v in [\"NNS\"]}\n",
    "noun_dict\n",
    "noun_list = list(noun_dict.keys())\n",
    "noun_list\n",
    "\n",
    "\n",
    "print(\"\")\n",
    "print(\"PRP personal pronoun\")\n",
    "personal_pronoun_dict = {k : v for k,v in text_pos_dict.items() if v in [\"PRP\"]}\n",
    "personal_pronoun_dict\n",
    "personal_pronoun_list = list(personal_pronoun_dict.keys())\n",
    "personal_pronoun_list\n",
    "\n",
    "\n",
    "print(\"\")\n",
    "print(\"PRP$ pronoun possessive (her mine my)\")\n",
    "possessive_pronoun_dict = {k : v for k,v in text_pos_dict.items() if v in [\"PRP$\"]}\n",
    "possessive_pronoun_dict\n",
    "possessive_pronoun_list = list(possessive_pronoun_dict.keys())\n",
    "possessive_pronoun_list\n",
    "\n",
    "\n",
    "print(\"RB RBR RBS adverb\")\n",
    "print(\"\")\n",
    "adverb_dict                = {k : v for k,v in text_pos_dict.items() if v in [\"RB\"] or v in [\"RBR\"] or v in [\"RBS\"]}\n",
    "adverb_dict\n",
    "adverb_list = list(adverb_dict.keys())\n",
    "adverb_list\n",
    "\n",
    "print(\"\")\n",
    "print(\"UH: interjection   Goodbye Goody Gosh Wow Jeepers Jee-sus\")\n",
    "interjection_dict = {k : v for k,v in text_pos_dict.items() if v in [\"UH\"]}\n",
    "interjection_dict\n",
    "interjection_list = list(interjection_dict.keys())\n",
    "interjection_list\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
