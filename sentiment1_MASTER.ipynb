{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP file for medical consultations\n",
    "\n",
    "## Part 1: prepare for loop over all .txt files\n",
    "1. set up Stanford dependency parser.\n",
    "2. create the positive & negative word lists:  general_inquirer_import.ipynb\n",
    "3. create functions for: 1: tokenize // 2: tokenize & stem\n",
    "## Part 2: LOOP - once for every .txt file\n",
    "Loop over every text file and calculate results\n",
    "## Part 3: After loop - summarise results\n",
    "And correlate with speaker type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Words to match on\n",
    "list_words1 = [\"i\", \"you\", \"we\", \"decide\", \"decision\", \"option\", \"options\"] \n",
    "list_words2 = []\n",
    "\n",
    "#### Speaker identifiers (these are followed by a colon at the start of each turn)\n",
    "speaker_ids = [\"P1\", \"N1\", \"D1\", \"D2\", \"R1\", \"R2\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert speaker identities into lower case (the text file will also be converted to lower case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_ids = [x.lower() for x in speaker_ids]\n",
    "speaker_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.parse.stanford import StanfordDependencyParser\n",
    "from nltk.parse.stanford import StanfordParser\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "import codecs\n",
    "from sklearn import feature_extraction\n",
    "import mpld3\n",
    "from matplotlib import pyplot as plt\n",
    "from itertools import zip_longest\n",
    "import spacy\n",
    "from collections import Counter\n",
    "from nltk import sent_tokenize\n",
    "from nltk import word_tokenize\n",
    "import io\n",
    "import xlrd\n",
    "import pprint\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python files used by this script\n",
    "\n",
    "**sentiment_tokenize.py** <br>\n",
    "**Output:** tokeniser functions e.g. tokenize_only // open_and_tokenize <br>\n",
    "\n",
    "**sentiment_general_inquirer_import.ipynb** <br>\n",
    "**Output:** produce 2 x lists of pos / neg words (called positive & negative)  <br>\n",
    "\n",
    "**count_num_speakers.py** <br>\n",
    "**Output:** 1x List: Num_speakers  <br>\n",
    "\n",
    "               \n",
    "**turns_per_speaker.py** <br>\n",
    "**Output:** 1x Dict = Num_turns <br>\n",
    "               \n",
    "\n",
    "**pos_neg_score_append.py** <br>\n",
    "**Output:** 2x Lists: positive_score_list.append(   <br>\n",
    "negative_score_list  <br>\n",
    "\n",
    "\n",
    "**pos_neg_words_set_freq.py** <br>\n",
    "**Output:** 2x Dicts = pos_FreqDist_all_dialogue  &  neg_FreqDist_all_dialogue   <br>\n",
    "2x Sets = pos_set_all_dialogue  &  neg_set_all_dialogue  <br>\n",
    "\n",
    "\n",
    "**parts_of_speech.ipynb**  <br>\n",
    "**Output: **  1x dictionary:  pos_consultation <br>\n",
    "\n",
    "\n",
    "**spacy_dep_parse.ipynb**  <br>\n",
    "**Output:** 1x dictionary:  dep_parse_consultation  <br>\n",
    "\n",
    "**count_med_terms.ipynb** <br>\n",
    "**Output:**  2x Dictionaries:  clin_concepts_number_per_consultation  clin_classes_type_number_per_consultation   <br>\n",
    "2x Sets:  clin_classes_set_final  clin_classes_set\n",
    "\n",
    "**loop_per_speaker.ipynb** <br>\n",
    "**Output:** 3x dictionaries: 1: per_speaker (contains most of per speaker results   <br>\n",
    "2: clin_concepts_number_per_consultation_speaker    3: clin_classes_type_number_per_consultation_speaker  <br>\n",
    "\n",
    "**loop_per_speaker_inner_loop.ipynb** <br>\n",
    "**Output:** Produces the POS / sentiment / active passive verb analysis for the foregoing file (loop_per_speaker.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### configure to display results of all content of cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### display the environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.environ['CLASSPATH'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methodology to create dictionary for results. i.e. link participant identifiers with metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Specify 2 x types of metrics:\n",
    "metrics = [\"Num_turns_\", \"Num_words_\"]\n",
    "\n",
    "#### List to store the metrics:\n",
    "metrics_speaker = []\n",
    "\n",
    "#### Combine the speaker IDs with the types of metrics:\n",
    "for a in metrics:\n",
    "    for x in speaker_ids:\n",
    "        full = a + x\n",
    "        metrics_speaker.append(full)\n",
    "\n",
    "#### Convert list into dictionary\n",
    "values = np.zeros(len(metrics_speaker))\n",
    "metrics_speaker_dict  = dict(zip(metrics_speaker, values))\n",
    "\n",
    "print(\"Print Dictionary Contents\")\n",
    "metrics_speaker_dict\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spacy preparation & demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"I cured the illness. The illness was cured by me. A red cup. I ran quickly.\")\n",
    "### Note: apply to a tokenised document:   new_doc = nlp(str(tokens))\n",
    "\n",
    "counter = 0\n",
    "\n",
    "while counter < len(list(doc.sents)):\n",
    "    for word in list(doc.sents)[counter]:\n",
    "        print(\"Word:\", word.text)     \n",
    "        print(\"Tag:\", word.tag_)\n",
    "        print(\"Head:\", word.head.text)\n",
    "        print( \"Dependency relation:\", word.dep_)\n",
    "        print( \"Children:\", list(word.children))\n",
    "        print(\"\")\n",
    "    counter += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# With this function in our toolbox, we can write a loop that prints out the subtree for each word in a sentence:\n",
    "\n",
    "def flatten_subtree(st):\n",
    "    return ''.join([w.text_with_ws for w in list(st)]).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the noun subjects relating to active and passive phrases.\n",
    "\n",
    "subjects_list = []\n",
    "for word in doc:\n",
    "    print(word.dep_)\n",
    "    if word.dep_ == ('nsubj'):\n",
    "        subjects_list.append(flatten_subtree(word.subtree))\n",
    "    if word.dep_ == ('nsubjpass'):\n",
    "        subjects_list.append(flatten_subtree(word.subtree))\n",
    "print(\"subjects_list: {}\".format(subjects_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Retrieve the noun subjects for active / passive phrases SEPARATELY (and count the number of each)\n",
    "\n",
    "nsubj_subjects = []\n",
    "nsubj_count    = 0\n",
    "for word in doc:\n",
    "    if word.dep_ == ('nsubj'):\n",
    "        nsubj_count += 1\n",
    "        nsubj_subjects.append(flatten_subtree(word.subtree))\n",
    "print(\"nsubj_subjects: {}\".format(nsubj_subjects))\n",
    "print(\"nsubj_count: {}\".format(nsubj_count))\n",
    "\n",
    "nsubjpass_subjects = []\n",
    "nsubjpass_count = 0\n",
    "for word in doc:\n",
    "    if word.dep_ == ('nsubjpass'):\n",
    "        nsubjpass_count += 1\n",
    "        nsubjpass_subjects.append(flatten_subtree(word.subtree))\n",
    "print(\"nsubjpass_subjects: {}\".format(nsubjpass_subjects))\n",
    "print(\"nsubjpass_count: {}\".format(nsubjpass_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo of lexical dispersion plot and collocation / hapaxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref = open('text4.txt')\n",
    "type(ref)\n",
    "raw = ref.read()\n",
    "type(raw)\n",
    "tokens = word_tokenize(raw)\n",
    "type(tokens)\n",
    "\n",
    "text = nltk.Text(tokens)\n",
    "type(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Produce a dispersion plot of some key words (can change the words as you like, e.g. include key clinical terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text.dispersion_plot([\"name\", \"sugar\", \"family\", \"exercise\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look at collocations (words that frequently occur beside each other)\n",
    "#### look at words that appear in a similar context to 'pain'\n",
    "#### look at shared contexts between the words medicine and pain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Collocations\")\n",
    "text.collocations()\n",
    "\n",
    "print()\n",
    "print(\"Words in similar contexts\")\n",
    "text.similar(\"sugar\")\n",
    "\n",
    "print()\n",
    "print(\"Common contexts\")\n",
    "text.common_contexts([\"sugar\", \"exercise\"])\n",
    "\t#How to examine just the contexts that are SHARED by TWO or more words?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examine amount of vocabulary and the repetition of vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Length of text\")\n",
    "len(text)\n",
    "\t#includes punctuation\n",
    "\t\n",
    "\n",
    "print()\n",
    "print(\"Size of vocabulary\")\n",
    "vocab_size = len(set(text))\n",
    "\t#size of vocabulary.\n",
    "vocab_size\n",
    "\n",
    "print()\n",
    "print(\"Lexical richness\")\n",
    "len(set(text)) / len(text)\n",
    "\t#LEXical richness.\n",
    "\t\n",
    "print(\"What percentage of text is taken up by the word 'exercise'?\")\n",
    "freq_word = 100 * text.count('exercise') / len(text)\n",
    "freq_word\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look at the most common words, the frequency of specific words; and words that only appear once ('hapaxes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist1 = nltk.FreqDist(text)  \n",
    "print(fdist1)  \n",
    "    ###<FreqDist with 19317 samples and 260819 outcomes>\n",
    "fdist1.most_common(50) \n",
    "\t#print 50 x most common tokens\n",
    "\t\t\n",
    "fdist1['doctor']\n",
    "\t#freq of particular word.\n",
    "\n",
    "fdist1.plot(50, cumulative=True)\n",
    "\t#cumulative frequency plot for top 50 words\n",
    "\t\n",
    "\t\n",
    "fdist1.hapaxes()\n",
    "\t#hapaxes ie words occuring only once\n",
    "\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count the number of questions: assume that each '?' indicates one question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in ['?']:\n",
    "    print('Number of questions: ', fdist1[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the 2 x lists of positive & negative words from General Inquirer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run -i sentiment_general_inquirer_import.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### print sample of first and last positive & negative words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Positive sample\")\n",
    "positive[:5]\n",
    "positive[-5:]\n",
    "\n",
    "print()\n",
    "print(\"Negative sample\")\n",
    "negative[:5]\n",
    "negative[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define tokenizer and stemmer functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run -i sentiment_tokenize.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assert tokenizer works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"1: tokenize_only\")\n",
    "test = \"Hi my friends\"\n",
    "tokenize_only(test)\n",
    "\n",
    "print(\"\")\n",
    "print(\"2: open_and_tokenize\")\n",
    "##################  loop over this & increment counter +1 each time.\n",
    "counter = 0\n",
    "folder  = \"sentiment_text_data\"\n",
    "open_and_tokenize(counter, folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run -i create_scores.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check number of text files in the directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_files = len(os.listdir(folder))\n",
    "print(\"Number of files in the directory: {}\".format(no_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Loop (i.e. for every text file in folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse content - use procedural coding to avoid issues re local / global variables in functional approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "\n",
    "# %run -i loop_per_speaker.ipynb\n",
    "\n",
    "#while counter < no_files:\n",
    "#    tokens = open_and_tokenize(counter, folder)\n",
    "#    counter += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#########################################################\n",
    "#########################################################  SCRIPTS WITH THEIR OWN INTERNAL LOOP.\n",
    "#########################################################\n",
    "\n",
    "############################################################  Medical terminology\n",
    "print(\"count_med_terms.ipynb\")\n",
    "%run -i  count_med_terms.ipynb\n",
    "\n",
    "\"\"\"\n",
    "Count the number of medical concepts:\n",
    "Output: 2x Dict:\n",
    " clin_concepts_number_per_consultation\n",
    " clin_classes_type_number_per_consultation\n",
    "\"\"\"\n",
    "\n",
    "############################################################  Loop over each speaker.\n",
    "print(\"loop_per_speaker.ipynb\")\n",
    "%run  -i loop_per_speaker.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "#########################################################\n",
    "#########################################################  SCRIPTS THAT GO INSIDE THE LOOP.\n",
    "#########################################################\n",
    "\n",
    "counter = 0\n",
    "\n",
    "while counter < no_files:\n",
    "    tokens = open_and_tokenize(counter, folder)\n",
    "\n",
    "    ############################################################  Number speakers & words & turns\n",
    "    %run -i count_num_speakers.py\n",
    "    \"\"\"\n",
    "    Output = List    Num_speakers\n",
    "    if the speaker id appears in the text ('tokens') - +1 to last value in list\n",
    "    \"\"\"\n",
    "\n",
    "    %run -i turns_per_speaker.py\n",
    "    \"\"\"\n",
    "    Output = dictionary    Num_turns\n",
    "    Each time speaker id appears at start of sentence: +1 to final value for the speaker \n",
    "    \"\"\"\n",
    "\n",
    "    ############################################################  Positive & Negative sentiment\n",
    "    %run -i pos_neg_score_append.py\n",
    "    \"\"\"\n",
    "    Output = 2xLists   positive_score_list & negative_score_list\n",
    "    count number of pos & neg words in each transcript 'tokens' - add 1 number to relevant list\n",
    "    \"\"\"\n",
    "    \n",
    "    %run -i pos_neg_words_set_freq.py\n",
    "    \"\"\"\n",
    "    Output = 2xLists & 2xDicts   \n",
    "    Lists:  pos_set_all_dialogue          neg_set_all_dialogue\n",
    "    Dicts:  pos_FreqDist_all_dialogue     neg_FreqDist_all_dialogue\n",
    "    1. count freq of all pos / neg words across all files.\n",
    "    2. create a set showing each pos / neg word once\n",
    "    \"\"\"\n",
    "\n",
    "    ############################################################  Parts of speech\n",
    "    %run -i parts_of_speech.ipynb\n",
    "    \"\"\"\n",
    "    Output: 1x dictionary\n",
    "    pos_consultation\n",
    "    \"\"\"\n",
    "    \n",
    "    ############################################################  Active & Passive phrases\n",
    "    %run -i spacy_dep_parse.ipynb\n",
    "    \"\"\"\n",
    "    Output: 1x dictionary\n",
    "    dep_parse_consultation\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    ############################################################  Count specific words & phrases specified in user input\n",
    "    %run -i count_words_phrases.py\n",
    " \n",
    "    counter += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: after loop: create summary metrics & show results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Per Consultation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### total no. pos / neg sentiment words per consultation\n",
    "print(positive_num)\n",
    "print(negative_num)\n",
    "\n",
    "\n",
    "### \n",
    "print(positive_score_list)\n",
    "print(negative_score_list)\n",
    "\n",
    "\n",
    "### no. of unique pos and neg sentiment words\n",
    "print(len(pos_set_all_dialogue))\n",
    "print(len(neg_set_all_dialogue))\n",
    "\n",
    "\n",
    "### sets of unique pos and neg sentiment words\n",
    "print(pos_set_all_dialogue)\n",
    "print(neg_set_all_dialogue)\n",
    "\n",
    "\n",
    "### Freq Dist pos and neg sentiment words\n",
    "pos_FreqDist_all_dialogue\n",
    "neg_FreqDist_all_dialogue\n",
    "\n",
    "\n",
    "### speakers & turns\n",
    "Num_speakers\n",
    "Num_turns\n",
    "\n",
    "\n",
    "### clin concepts & classes\n",
    "clin_concepts_number_per_consultation\n",
    "clin_classes_type_number_per_consultation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print results of: Positive & Negative word set, Pos & Neg frequency distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_set_all_dialogue\n",
    "%run -i sentiment_pos_neg_results.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print results of: Turn taking: number of words and sentences per speaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run sentiment_turn_taking_results.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Print results of: Active & Passive noun dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run sentiment_active_passive.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Per Speaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of clinical concepts per consultation\")\n",
    "clin_concepts_number_per_consultation_speaker\n",
    "\n",
    "print(\"How many times each classification appeared in each transcript\")\n",
    "clin_classes_type_number_per_consultation_speaker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Active & passive verbs /// Parts of speech (adjective, adverb, verb, personal pronoun) \n",
    "    ###     /// Positive and negative sentiment words\n",
    "per_speaker"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
