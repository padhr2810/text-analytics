{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal file.\n",
    "\n",
    "## Part 1: prepare for loop over all .txt files\n",
    "1. import dependencies\n",
    "2. set up Stanford dependency parser.\n",
    "3. create the positive & negative word lists:  general_inquirer_import.ipynb\n",
    "4. create functions for: 1: tokenize // 2: tokenize & stem\n",
    "5. create Empty Lists for number of positive words, negative words, active voice, passive voice, etc.\n",
    "## Part 2: LOOP - once for every .txt file\n",
    "1. Sentiment analysis (open & tokenize text file) <br>\n",
    "ii  Count aggregate number of all positive and negative items <br>\n",
    "iii Show all pos and neg words for this text file. <br>\n",
    "iv  construct set // frequency of all pos and neg words in all of the text files. <br>\n",
    "2. Turn-taking <br>\n",
    "ii.  No. participants <br>\n",
    "iii. No. turns per speaker (i.e. just count lines starting with N1 etc) <br>\n",
    "iv.  Per speaker: run another loop (sentiment3_each_speaker.ipynb) <br>\n",
    "\n",
    "## Part 3: After loop - summarise results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.parse.stanford import StanfordDependencyParser\n",
    "from nltk.parse.stanford import StanfordParser\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "import codecs\n",
    "from sklearn import feature_extraction\n",
    "import mpld3\n",
    "from matplotlib import pyplot as plt\n",
    "from itertools import zip_longest\n",
    "import spacy\n",
    "from collections import Counter\n",
    "from nltk import sent_tokenize\n",
    "from nltk import word_tokenize\n",
    "import io\n",
    "import pandas as pd\n",
    "import xlrd\n",
    "import pprint\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### configure to display results of all cell contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### display the environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Padhraig\\OneDrive\\Python\\inca python code\\Sentiment_Analysis;C:\\Users\\Padhraig\\Documents\\INCA data\\inca python code\\Sentiment_Analysis;C:\\ProgramData\\Anaconda3;C:\\ProgramData\\Anaconda3\\Scripts;C:\\ProgramData\\Anaconda3\\pkgs\\python-3.6.2-h6679aeb_11;C:\\ProgramData\\Anaconda3\\Lib\\site-packages\\stanford-corenlp-full-2017-06-09;C:\\Users\\Padhraig\\OneDrive\\Python\\inca python code;C:\\Users\\Padhraig\\OneDrive\\Python\\python-conversational-alignment-master\\stanford-postagger-2015-04-20;\n"
     ]
    }
   ],
   "source": [
    "print(os.environ['CLASSPATH'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create list of participant identifiers.\n",
    "E.g. if utterances / paragraphs start with: Doctor1\n",
    "                                            Patient1\n",
    "Create list as:\n",
    "speaker_list = [Doctor1:, Patient1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "speaker_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### JUST FYI LITTLE DEMO OF HOW TO JOIN A SUFFIX ONTO A VARIABLE STEM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List 1\n",
      "Num_turns_\n",
      "Num_words_\n",
      "\n",
      "List 2\n",
      "P1\n",
      "N1\n",
      "D1\n",
      "D2\n",
      "R1\n",
      "R2\n",
      "\n",
      "New List\n",
      "Num_words_P1\n",
      "Num_words_N1\n",
      "Num_words_D1\n",
      "Num_words_D2\n",
      "Num_words_R1\n",
      "Num_words_R2\n"
     ]
    }
   ],
   "source": [
    "%run -i suffix_demo.py\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New List\n",
      "Num_turns_P1\n",
      "Num_turns_N1\n",
      "Num_turns_D1\n",
      "Num_turns_D2\n",
      "Num_turns_R1\n",
      "Num_turns_R2\n",
      "Num_words_P1\n",
      "Num_words_N1\n",
      "Num_words_D1\n",
      "Num_words_D2\n",
      "Num_words_R1\n",
      "Num_words_R2\n",
      "Dictionary effort\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Num_turns_P1': 'b'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Print Dictionary Contents\n",
      "{'Num_turns_P1': 'b'}\n"
     ]
    }
   ],
   "source": [
    "LIST1 = [\"Num_turns_\", \"Num_words_\"]\n",
    "LIST2 = [\"P1\", \"N1\", \"D1\", \"D2\", \"R1\", \"R2\"]\n",
    "\n",
    "newlist = []\n",
    "\n",
    "for a in LIST1:\n",
    "    for x in LIST2:\n",
    "        full = a + x\n",
    "        newlist.append(full)\n",
    "\n",
    "print(\"\")\n",
    "print(\"New List\")\n",
    "for x in newlist:\n",
    "    print(x)\n",
    "\n",
    "print(\"Dictionary effort\")\n",
    "diction = dict(zip(newlist, \"b\"))\n",
    "diction\n",
    "\n",
    "print(\"Print Dictionary Contents\")\n",
    "pprint.pprint(diction)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spacy preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"I cured the illness. The illness was cured by me.\")\n",
    "\n",
    "for word in list(doc.sents)[1]:\n",
    "    print(\"Word:\", word.text)     \n",
    "    print(\"Tag:\", word.tag_)\n",
    "    print(\"Head:\", word.head.text)\n",
    "    print( \"Dependency relation:\", word.dep_)\n",
    "    print( \"Children:\", list(word.children))\n",
    "    print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def flatten_subtree(st):\n",
    "    return ''.join([w.text_with_ws for w in list(st)]).strip()\n",
    "\n",
    "# With this function in our toolbox, we can write a loop that prints out the subtree for each word in a sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects_list = []\n",
    "for word in doc:\n",
    "    if word.dep_ in ('nsubj', 'nsubjpass'):\n",
    "        subjects_list.append(flatten_subtree(word.subtree))\n",
    "subjects_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsubj_subjects = []\n",
    "nsubj_count    = 0\n",
    "for word in doc:\n",
    "    if word.dep_ in ('nsubj'):\n",
    "        nsubj_count += 1\n",
    "        nsubj_subjects.append(flatten_subtree(word.subtree))\n",
    "nsubj_subjects\n",
    "nsubj_count\n",
    "\n",
    "nsubjpass_subjects = []\n",
    "nsubjpass_count = 0\n",
    "for word in doc:\n",
    "    if word.dep_ in ('nsubjpass'):\n",
    "        nsubjpass_count += 1\n",
    "        nsubjpass_subjects.append(flatten_subtree(word.subtree))\n",
    "nsubjpass_subjects\n",
    "nsubjpass_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the 2 x lists of positive & negative words from General Inquirer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run sentiment5_general_inquirer_import.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### print sample of first and last positive & negative words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Positive sample\")\n",
    "positive[:5]\n",
    "positive[-5:]\n",
    "\n",
    "print()\n",
    "print(\"Negative sample\")\n",
    "negative[:5]\n",
    "negative[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define tokenizer and stemmer functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run -i sentiment_tokenize.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assert it is working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"tokenize_only\")\n",
    "test = \"Hi my friends\"\n",
    "tokenize_only(test)\n",
    "\n",
    "print(\"open_and_tokenize\")\n",
    "##################  loop over this & increment counter +1 each time.\n",
    "counter = 0\n",
    "folder  = \"sentiment_text_data\"\n",
    "open_and_tokenize(counter, folder)\n",
    "\n",
    "# print(\"tokenize_and_stem\")\n",
    "# tokenize_and_stem(test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create empty lists to contain the results\n",
    "\n",
    "1. set of all positive words \n",
    "2. set of all negative words\n",
    "3. frequency of pos words (i.e. frequency distribution)\n",
    "4. frequency of neg words (i.e. frequency distribution)\n",
    "5. pos score for each dialogue\n",
    "6. neg score for each dialogue\n",
    "7. no. of speakers for each dialogue\n",
    "8. no. turns per speaker\n",
    "9. no. words per speaker\n",
    "10. Perc. words that are positive\n",
    "11. Perc. words that are negative\n",
    "12. Perc. words that are active\n",
    "13. Perc. words that are passive\n",
    "14. mean no. words per turn\n",
    "15. total turns per dialogue\n",
    "16. total words per dialogue\n",
    "17. Perc. total turns uttered by each speaker\n",
    "18. Perc. total words uttered by each speaker\n",
    "19. ACtive word per speaker\n",
    "20. Passive word per speaker\n",
    "21. Positive word per speaker\n",
    "22. Negative word per speaker\n",
    "23. adverbs per speaker (e.g. Women elaborate more than men - women use adverbs more)\n",
    "24. adjectives per speaker\n",
    "25. freq of word \"I\" per speaker.\n",
    "26. mean sentence length per speaker (do Men use shorter sentences? some research suggests this?)\n",
    "27. count medical terminology / concepts / jargon per speaker\n",
    "28. mirroring of communication style (e.g. gender effects)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run -i sentiment_create_scores.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### show some of these empty lists that i created:\n",
    "\n",
    "positive_score_list\n",
    "pos_set_all_dialogue\n",
    "Num_turns_P1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse content - use procedural code to avoid issues re local / global variables in functional programming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##  %run -i sentiment_build_functions.py\n",
    "\n",
    "counter = 0\n",
    "\n",
    "# while counter < no_files:\n",
    "while counter < 1:\n",
    "    tokens = open_and_tokenize(counter, folder)\n",
    "    \n",
    "    %run -i pos_neg_lists_append.py\n",
    "    \"\"\"\n",
    "    for content of 'tokens' - add 1 number to positive_score_list & negative_score_list\n",
    "    \"\"\"\n",
    "    \n",
    "    %run -i pos_neg_words_this_file.py\n",
    "    \"\"\"\n",
    "    for content of 'tokens' - return 4x Lists - pos_in_this, neg_in_this, pos_freq_in_this, neg_freq_in_this\n",
    "    \"\"\"\n",
    "    \n",
    "    %run -i extend_set_all_pos_neg_words.py\n",
    "    \"\"\"\n",
    "    for content of 'tokens' - return 4x lists - pos_set_all_dialogue,  pos_freq_all_dialogue,  neg_set_all_dialogue, neg_freq_all_dialogue\n",
    "    these are the sets & freq of pos & neg words in one list, collectively for all texts.\n",
    "    \"\"\"\n",
    "    \n",
    "    %run -i count_num_speakers.py\n",
    "    \"\"\"\n",
    "    for content of 'tokens' - add 1 number to end of list \"Num_speakers\"\n",
    "    \"\"\"\n",
    "    \n",
    "    %run -i turns_per_speaker.py\n",
    "    \"\"\"\n",
    "    For tokens - add 1 x number to variable for each speaker e.g. \"Num_turns_P1\"\n",
    "    \"Num_turns_P1\" were defined already\n",
    "    \"\"\"\n",
    "    \n",
    "    %run -i align_package_nickduran_pr.py\n",
    "    \"\"\"\n",
    "    Run Nick Duran's 'align' package on the data.\n",
    "    \"\"\"\n",
    "    counter += 1\n",
    "\n",
    "\n",
    "\n",
    "print(positive_num)\n",
    "print(negative_num)\n",
    "print(positive_score_list)\n",
    "print(negative_score_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assert the functions work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check number of files present in the directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print()\n",
    "print(\"Check how many files in the directory\")\n",
    "### Check how many files in the directory\n",
    "no_files = len(os.listdir(folder))\n",
    "no_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Loop (i.e. repeat for every text file in folder)\n",
    "### open the first text file: tokenise it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### First: define name of file to open\n",
    "file = os.listdir(\"sentiment_text_data\")[0]\n",
    "type(file)\n",
    "file\n",
    "\n",
    "print()\n",
    "print(\"Open file and display 'type'\")\n",
    "f = open(file)\n",
    "type(f)\n",
    "\n",
    "print()\n",
    "print(\"Read file and display 'type'\")\n",
    "f2   = f.read()\n",
    "type(f2)\n",
    "\n",
    "print()\n",
    "print(\"Tokenise file and display 'type'\")\n",
    "### Tokenize the text:\n",
    "tokens = tokenize_only(f2)\n",
    "type(tokens)\n",
    "\n",
    "################################################## run the functions from the other file.\n",
    "loop_functions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "counter = 0\n",
    "\n",
    "# while counter < no_files:\n",
    "open_and_tokenize()\n",
    "tokens[:10]\n",
    "tokens[-10:]\n",
    "\n",
    "pos_neg_lists_append_func()\n",
    "\n",
    "pos_in_this, neg_in_this,  pos_freq_in_this, neg_freq_in_this  =   pos_neg_words_this_file_func()\n",
    "\n",
    "print(\"1\")\n",
    "pos_set_all_dialogue\n",
    "print(\"2\")\n",
    "pos_set_all_dialogue.extend(pos_in_this)\n",
    "print(\"3\")\n",
    "pos_set_all_dialogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "extend_set_all_pos_neg_words_func()\n",
    "\n",
    "\"\"\"\n",
    "count_num_speakers_func()\n",
    "turns_per_speaker_func()   \n",
    "counter = counter + 1\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: after loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Positive & Negative word set, Pos & Neg frequency distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_set_all_dialogue\n",
    "%run -i sentiment_pos_neg_results.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentiment_more_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Turn taking: number of words and sentences per speaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run sentiment_turn_taking_results.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Active & Passive noun dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run sentiment_active_passive.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
